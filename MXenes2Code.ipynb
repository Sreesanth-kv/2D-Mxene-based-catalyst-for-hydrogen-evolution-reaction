{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Importing modules"
      ],
      "metadata": {
        "id": "TVAL4XUSEZMH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJXtYlEbEFPh"
      },
      "outputs": [],
      "source": [
        "#IMPORTING LIBRARIES\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import joblib\n",
        "\n",
        "#IMPORTING DATAPREPROCESSING TOOLS\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "#IMPORTING MODELS\n",
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "from sklearn.linear_model import ElasticNet\n",
        "from sklearn.ensemble import GradientBoostingRegressor \n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "#IMPORTING CROSS VALIDATION TOOLS\n",
        "from sklearn.model_selection import KFold\t\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import cross_val_predict\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "#FEATURE TOOLS\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.inspection import permutation_importance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#NAMING MODELS\n",
        "abr = AdaBoostRegressor()\n",
        "en = ElasticNet()\n",
        "gbr = GradientBoostingRegressor()\n",
        "knr = KNeighborsRegressor()\n",
        "kr = KernelRidge()\n",
        "las = Lasso()\n",
        "pls = PLSRegression()\n",
        "rfr = RandomForestRegressor()\n",
        "rdg = Ridge()\n",
        "\n",
        "allmodels=[abr, en, gbr, knr, kr, las, pls, rfr, rdg]"
      ],
      "metadata": {
        "id": "FEqjDJ1sHsad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LOADING DATA\n",
        "df1 = pd.read_csv('dataset.csv')\n",
        "df2 = pd.read_csv('deltaG_H.csv')\n",
        "df = pd.merge(df1, df2, on = 'Mxenes')\n",
        "#dropping unnecessary columns\n",
        "df = df.drop(['G_X 17'], axis=1)"
      ],
      "metadata": {
        "id": "oBVP-NvXGTed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Feature Subsets generation"
      ],
      "metadata": {
        "id": "rqjnoSO4ynZp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#DIVIDING DATASET INTO FEATURE SETS\n",
        "set1_trainx = df.iloc[:,0:62]                                       #SET-1 ATOMISTIC FEATURES \n",
        "set2_trainx = df.iloc[:,62:83]                                      #SET-2 SURFACE (STRUCTURAL + ELECTRONIC) FEATURES\n",
        "set3_trainx = df.iloc[:,83:-1]                                      #SET-3 STATISTICAL FEATURES \n",
        "set4_trainx = pd.concat([set1_trainx, set2_trainx], axis = 1)       #SET-4 = 1+2\n",
        "set5_trainx = pd.concat([set1_trainx, set3_trainx], axis = 1)       #SET-5 = 1+3\n",
        "set6_trainx = pd.concat([set2_trainx, set3_trainx], axis = 1)       #SET-6 = 2+3\n",
        "set7_trainx = df.iloc[:,:-1]                                        #SET-7 = 1+2+3 "
      ],
      "metadata": {
        "id": "Ut6Bsj60G-jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cross Validation for all feature sets"
      ],
      "metadata": {
        "id": "oPTAAIx8ymaB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#SET 1-7 Cross validation\n",
        "#rename set1 each time for each set CV data\n",
        "set_r2 = []\n",
        "set_mae = []\n",
        "\n",
        "for model in allmodels:\n",
        "  train_x = set1_trainx \n",
        "  train_y = df.iloc[:,-1]\n",
        "  sf = MinMaxScaler()\n",
        "  enc = OneHotEncoder()\n",
        "  numerical = train_x.select_dtypes(exclude='bool').columns\n",
        "  categorical = train_x.select_dtypes(include='object').columns\n",
        "  t = [(\"cat\", enc, categorical), (\"num\", sf, numerical)]\n",
        "  ct = ColumnTransformer(transformers=t)\n",
        "  train_x_norm = ct.fit_transform(train_x)\n",
        "  model.fit(train_x_norm, train_y)\n",
        "  \n",
        "  print(model)\n",
        "  cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "  score = cross_val_score(model, train_x[selected_features], train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "  score2 = cross_val_score(model, train_x[selected_features], train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "  print('avg MAE:', sum(score)/len(score), 'avg R2:', sum(score2)/len(score2))\n",
        "\n",
        "  set_mae.append(round(score,3))\n",
        "  set_r2.append(round(score2,3))\n",
        "\n",
        "  model.fit(train_x, train_y)\n",
        "  predictions = model.predict(train_x).flatten()\n",
        "  \n",
        "  a = plt.axes(aspect='equal')\n",
        "  plt.scatter(train_y, predictions)\n",
        "  plt.xlabel('True Values [gibbs free energy]')\n",
        "  plt.ylabel('Predictions [gibbs free energy]')\n",
        "  lims = [-5,5]\n",
        "  plt.xlim(lims)\n",
        "  plt.ylim(lims)\n",
        "  \n",
        "  fig = plt.plot(lims, lims)\n",
        "  plt.show()\n",
        "  print('------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "set1cv = pd.DataFrame()\n",
        "set1cv['model'] = allmodels\n",
        "set1cv['r2'] = set_r2\n",
        "set1cv['mae'] = set_mae\n",
        "set1cv.to_csv('set1.csv', index = False)\n",
        "del set_r2\n",
        "del set_mae"
      ],
      "metadata": {
        "id": "1qqVXZESjp0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Test-Train Verification on Set-7"
      ],
      "metadata": {
        "id": "97Un3ga8ywQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = df\n",
        "X = dataset.iloc[:, :-1].values\n",
        "y = dataset.iloc[:, -1].values\n",
        "\n",
        "train_x, test_x, train_y, test_y = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
        "for model in allmodels:\n",
        "  r2list = []\n",
        "  maelist = []\n",
        "  r2testlist = []\n",
        "  maetestlist = [] \n",
        "  print(model)\n",
        "  \n",
        "  model.fit(train_x, train_y)\n",
        "  train_y_pred = model.predict(train_x)\n",
        "  r2 = r2_score(train_y, train_y_pred)\n",
        "  maer = abs(mae(train_y, train_y_pred))\n",
        "  r2list.append(r2)\n",
        "  maelist.append(maer)\n",
        "  test_y_pred = model.predict(test_x)\n",
        "  r2test = r2_score(test_y, test_y_pred)\n",
        "  maertest = abs(mae(test_y, test_y_pred))\n",
        "  r2testlist.append(r2test)\n",
        "  maetestlist.append(maertest)\n",
        "  print('------------------------------------------------------------------------------------------------------------------')\n",
        "\n",
        "set7tt = pd.DataFrame()\n",
        "set7tt['model'] = str(model) \n",
        "set7tt['r2'] = r2list\n",
        "set7tt['mae'] = maelist\n",
        "set7tt['test r2'] =  r2testlist\n",
        "set7tt['test mae'] = maetestlist\n",
        "set7tt.to_csv('set7 test train data.csv')\n",
        "del r2list\n",
        "del maelist\n",
        "del r2testlist\n",
        "del maetestlist"
      ],
      "metadata": {
        "id": "QGHrGbl6vvIw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Recursive Feature Elimination"
      ],
      "metadata": {
        "id": "fhz-8-3Ly7Z-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#RFE ON BEST MODELS FOR FEATURE SELECTION\n",
        "train_x = df.iloc[:,:-1]\n",
        "train_y = df.iloc[:,-1]\n",
        "numerical = train_x.select_dtypes(exclude='bool').columns\n",
        "categorical = train_x.select_dtypes(include='object').columns\n",
        "\n",
        "t = [(\"cat\", enc, categorical), (\"num\", sf, numerical)]\n",
        "ct = ColumnTransformer(transformers=t)\n",
        "train_x_norm = ct.fit_transform(train_x)"
      ],
      "metadata": {
        "id": "cdZty6sYkxHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RFE on RFR Model\n",
        "#running a 'for' loop to check best metrics scores by varying the number of features\n",
        "numf = [60, 55, 50, 45, 40, 35, 30, 25, 20, 15, 10]\n",
        "model = RandomForestRegressor()\n",
        "\n",
        "for i in numf:\n",
        "  selector = RFE(model, n_features_to_select=i, step=10, verbose=0, importance_getter='auto')\n",
        "  selector.fit(train_x_norm, train_y)\n",
        "  selector_support = selector.get_support()\n",
        "  selected_features = train_x.loc[:, selector_support].columns.tolist()\n",
        "  print(selected_features)\n",
        "  cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "  score = cross_val_score(model, train_x[selected_features], train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "  score2 = cross_val_score(model, train_x[selected_features], train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "  mae = (sum(score)/len(score))\n",
        "  r2 = (sum(score2)/len(score2))\n",
        "  print('num of features: ', i)\n",
        "  print('MAE:', round(mae,3))\n",
        "  print('R2:', round(r2,3))\n",
        "  print('--------------------------------------------------------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "id": "afCvZtASzefB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best metrics score when number of features were in between 20-30. Thus, checking in that range\n",
        "\n",
        "numf = [30, 29, 28, 27, 26, 25, 24, 23, 22, 21, 20]\n",
        "for i in numf:\n",
        "  selector = RFE(model, n_features_to_select=i, step=10, verbose=0, importance_getter='auto')\n",
        "  selector.fit(train_x_norm, train_y)\n",
        "  selector_support = selector.get_support()\n",
        "  selected_features = train_x.loc[:, selector_support].columns.tolist()\n",
        "  print(selected_features)\n",
        "  cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "  score = cross_val_score(model, train_x[selected_features], train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "  score2 = cross_val_score(model, train_x[selected_features], train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "  mae = (sum(score)/len(score))\n",
        "  r2 = (sum(score2)/len(score2))\n",
        "  print('num of features: ', i)\n",
        "  print('MAE:', round(mae,3))\n",
        "  print('R2:', round(r2,3))\n",
        "  print('--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "mqfayCVd0QNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#RFE on GBR model\n",
        "#running a 'for' loop to check best metrics scores by varying the number of features\n",
        "model = GradientBoostingRegressor()\n",
        "numf = [60, 55, 50, 45, 40, 35, 30, 25, 20, 15, 10]\n",
        "\n",
        "for i in numf:\n",
        "  selector = RFE(model, n_features_to_select=i, step=10, verbose=0, importance_getter='auto')\n",
        "  selector.fit(train_x_norm, train_y)\n",
        "  selector_support = selector.get_support()\n",
        "  selected_features = train_x.loc[:, selector_support].columns.tolist()\n",
        "  print(selected_features)\n",
        "  cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "  score = cross_val_score(model, train_x[selected_features], train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "  score2 = cross_val_score(model, train_x[selected_features], train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "  mae = (sum(score)/len(score))\n",
        "  r2 = (sum(score2)/len(score2))\n",
        "  print('num of features: ', i)\n",
        "  print('MAE:', round(mae,3))\n",
        "  print('R2:', round(r2,3))\n",
        "  print('--------------------------------------------------------------------------------------------------------------------------')\n"
      ],
      "metadata": {
        "id": "psoa4WYse7Jv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#best metrics score when number of features were in between 20-30. Thus, checking in that range\n",
        "\n",
        "numf = [35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 25]\n",
        "for i in numf:\n",
        "  selector = RFE(model, n_features_to_select=i, step=10, verbose=0, importance_getter='auto')\n",
        "  selector.fit(train_x_norm, train_y)\n",
        "  selector_support = selector.get_support()\n",
        "  selected_features = train_x.loc[:, selector_support].columns.tolist()\n",
        "  print(selected_features)\n",
        "  cv = KFold(n_splits=10, shuffle=True, random_state=None)\n",
        "  score = cross_val_score(model, train_x[selected_features], train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "  score2 = cross_val_score(model, train_x[selected_features], train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "  mae = (sum(score)/len(score))\n",
        "  r2 = (sum(score2)/len(score2))\n",
        "  print('num of features: ', i)\n",
        "  print('MAE:', round(mae,3))\n",
        "  print('R2:', round(r2,3))\n",
        "  print('--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------')"
      ],
      "metadata": {
        "id": "-fuuS8O0lD5f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Hyperparameter optimization"
      ],
      "metadata": {
        "id": "4MS2KKFDzrad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameter optimization (tuning) for RFR Model\n",
        "\n",
        "n_estimators = [20, 40, 60, 80, 100, 200, 300, 400, 600, 700, 800, 1000]\n",
        "max_features = [None, 'sqrt', 'log2', 25, 15, 10, 5, 2]\n",
        "max_depth = [10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 1000, 5000, 10000]\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)\n",
        "\n",
        "model = RandomForestRegressor()\n",
        "\n",
        "#proplist = ['atomic no. m', 'valance electron t', 'electron affinity (eV) t', 'MP (in degree C) t', 'valance electron avg', 'BP (in degree C) t', 'layer thickness', 'bondlength m-t', 'bondlength m-x', 'dbandcenter', 'x-x sq', 'm-m2 sq', 't-t2 sq', 't-t', 'm-m', 'x-x', 'm-m2', 'wfn', 'max bl', 'dbandcenterpr', 'dbandcenter var', 'ea var', 'dbandcenter std', 'distance x-x var', 'gibbs free energy']\n",
        "proplist = ['N_M', 'VE_T', 'EA_T', 'MP_T', 'VE avg', 'BP_T', 'LT', 'l_M-T', 'l_M-X', 'dbc', 'd_X-X sq', 'd_M-M2 sq', 'd_T1-T2 sq', 'd_T-T', 'd_M-M', 'd_X-X', 'd_M-M2', 'WF', 'l_max', 'dbc avg', 'dbc var', 'EA_T var', 'dbc std', 'd_X-X var', 'deltaG_H']\n",
        "\n",
        "#proplist =  selected_features + target_property\n",
        "df = df[proplist]\n",
        "train_x = df.iloc[:,:-1]\n",
        "train_y = df.iloc[:,-1]\n",
        "sf = MinMaxScaler()\n",
        "enc = OneHotEncoder()\n",
        "numerical = train_x.select_dtypes(exclude='bool').columns\n",
        "categorical = train_x.select_dtypes(include='object').columns\n",
        "t = [(\"cat\", enc, categorical), (\"num\", sf, numerical)]\n",
        "ct = ColumnTransformer(transformers=t)\n",
        "train_x_norm = ct.fit_transform(train_x)\n",
        "\n",
        "rf_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=2, random_state=0, scoring='r2', return_train_score=True)\n",
        "# Fit the random search model\n",
        "rf_random.fit(train_x_norm, train_y)\n",
        "print(rf_random.best_params_)"
      ],
      "metadata": {
        "id": "nhiIKHs-o295"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#hyperparameter optimization (tuning) for GBR model\n",
        "\n",
        "learning_rate=[0.001, 0.005, 0.01, 0.015, 0.1, 0.5, 1]\n",
        "n_estimators = [20, 40, 60, 80, 100, 200, 300, 400, 600, 700, 800, 1000]\n",
        "max_features = [None, 'sqrt', 'log2', 25, 15, 10, 5, 2]\n",
        "max_depth = [3, 5, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 1000, 5000, 10000]\n",
        "min_samples_split = [2, 5, 10]\n",
        "min_samples_leaf = [1, 2, 4]\n",
        "bootstrap = [True, False]\n",
        "\n",
        "random_grid = {'n_estimators': n_estimators,\n",
        "               'max_features': max_features,\n",
        "               'max_depth': max_depth,\n",
        "               'min_samples_split': min_samples_split,\n",
        "               'min_samples_leaf': min_samples_leaf,\n",
        "               'bootstrap': bootstrap}\n",
        "print(random_grid)\n",
        "\n",
        "model = GradientBoostingRegressor()\n",
        "\n",
        "#selected_features_gbr = ['metallic radii (pm) m', 'valance electron t', 'first ionization potential (eV) t', 'electron affinity (eV) t', 'MP (in degree C) t', 'BP (in degree C) t', 'electronegativity t', 'cohesive_en', 'layer thickness', 'bondlength m-t', 'bondlength m-x', 'dbandcenter', 'distance x-x', 'distance m-m2', 'nearest neighbor', 'work_fn', 'min bondlength', 'max bondlength', 'electron affinity var', 'cohesive_en std', 'distance m2-t avg', 'dbandcenter avg', 'dbandcenter var', 'dbandcenter std', 'distance t-t2 std', 'wfn std', 'valance electron t avg', 'normalized surface wt', 't-t sq', 'm-m2 sq', 'gibbs free energy']\n",
        "proplist = ['r_M', 'VE_T', 'IE_T', 'EA_T', 'MP_T', 'BP_T', 'EA_T', 'W', 'E_coh', 'LT', 'l_M-T', 'l_M-X', 'dbc ', 'd_X-X', 'd_M-M2', 'd_NN', 'WF', 'l_min', 'l_max', 'EA var', 'E_coh std', 'l_M2-T avg', 'dbc avg', 'dbc var', 'dbc std', 'd_T1-T2 std', 'WF std', 'VE_T avg', 'd_T-T sq', 'd_M-M2 sq', 'deltaG_H']\n",
        "#proplist =  selected_features + target_property\n",
        "df = df[proplist]\n",
        "train_x = df.iloc[:,:-1]\n",
        "train_y = df.iloc[:,-1]\n",
        "sf = MinMaxScaler()\n",
        "enc = OneHotEncoder()\n",
        "numerical = train_x.select_dtypes(exclude='bool').columns\n",
        "categorical = train_x.select_dtypes(include='object').columns\n",
        "t = [(\"cat\", enc, categorical), (\"num\", sf, numerical)]\n",
        "ct = ColumnTransformer(transformers=t)\n",
        "train_x_norm = ct.fit_transform(train_x)\n",
        "\n",
        "# search across 100 different combinations, and use all available cores\n",
        "gbr_random = RandomizedSearchCV(estimator = model, param_distributions = random_grid, n_iter = 100, cv = 10, verbose=2, random_state=0, scoring='r2', return_train_score=True)\n",
        "# Fit the random search model\n",
        "gbr_random.fit(train_x_norm, train_y)\n",
        "print(gbr_random.best_params_)"
      ],
      "metadata": {
        "id": "Mu-GHCQpnNO4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Target property prediction"
      ],
      "metadata": {
        "id": "mSFPWdbMyS2G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#FINAL RFR MODEL\n",
        "model = RandomForestRegressor(n_estimators=1000, min_samples_split=5, min_samples_leaf=2, max_features=15, max_depth=500, bootstrap= True)\n",
        "model.fit(train_x_norm, train_y)\n",
        "\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "score = cross_val_score(model, train_x, train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "score2 = cross_val_score(model, train_x, train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "s1 = sum(score)/len(score)\n",
        "s2 = sum(score2)/len(score2)\n",
        "print(round(s1,3), round(s2,3))"
      ],
      "metadata": {
        "id": "dTr8frQEsNYQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVING MODEL (for further use)\n",
        "joblib.dump(model, 'rfr_model.joblib')\n",
        "\n",
        "#PREDICTING GIBBS FREE ENERGY FOR 4500 CATALYSTS \n",
        "selected_features_rfr = ['N_M', 'V_T', 'EA_T', 'MP_T', 'V avg', 'BP_T', 'LT', 'l_M-T', 'l_M-X', 'dbc', 'd_X-X sq', 'd_M-M2 sq', 'd_T1-T2 sq', 'd_T-T', 'd_M-M', 'd_X-X', 'd_M-M2', 'WF', 'L_max', 'dbc avg', 'dbc var', 'EA_T var', 'dbc std', 'd_X-X var']\n",
        "x_all = df1[selected_features_rfr] #feature dataset\n",
        "model.fit(train_x, train_y)\n",
        "predictions_rfr = model.predict(x_all) #predicted target property "
      ],
      "metadata": {
        "id": "x5ZPHR-AwsDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#FINAL GBR MODEL\n",
        "model = GradientBoostingRegressor(n_estimators= 400, min_samples_split= 2, min_samples_leaf= 10, max_features= 'sqrt', max_depth= 1000, learning_rate= 0.015)\n",
        "model.fit(train_x_norm, train_y)\n",
        "\n",
        "cv = KFold(n_splits=10, shuffle=True, random_state=0)\n",
        "score = cross_val_score(model, train_x, train_y, scoring='neg_mean_absolute_error',cv=cv, n_jobs=None)\n",
        "score2 = cross_val_score(model, train_x, train_y, scoring='r2',cv=cv, n_jobs=None)\n",
        "s1 = sum(score)/len(score)\n",
        "s2 = sum(score2)/len(score2)\n",
        "print(s1, s2)\n",
        "print(round(s1,3), round(s2,3))"
      ],
      "metadata": {
        "id": "9uJ04weAsvcX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#SAVING MODEL (for further use)\n",
        "joblib.dump(model, 'gbr_model.joblib')\n",
        "\n",
        "#PREDICTING GIBBS FREE ENERGY FOR 4500 CATALYSTS\n",
        "selected_features_gbr = ['r_M', 'V_T', 'IE_T', 'EA_T', 'MP_T', 'BP_T', 'EA_T', 'W', 'E_coh', 'LT', 'l_M-T', 'l_M-X', 'dbc ', 'd_X-X', 'd_M-M2', 'd_NN', 'WF', 'l_min', 'l_max', 'EA var', 'E_coh std', 'l_M2-T avg', 'dbc avg', 'dbc var', 'dbc std', 'd_T1-T2 std', 'WF std', 'V_T avg', 'd_T-T sq', 'd_M-M2 sq']\n",
        "\n",
        "x_all = df1[selected_features_gbr] #feature dataset\n",
        "model.fit(train_x, train_y)\n",
        "predictions_gbr = model.predict(x_all) #predicted target property"
      ],
      "metadata": {
        "id": "QGJdJzMJtZRi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Finding Key Descriptors"
      ],
      "metadata": {
        "id": "wf6UGLSWGA7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "result = permutation_importance(\n",
        "    model, train_x, train_y, n_repeats=10, random_state=42, n_jobs=2\n",
        ")\n",
        "elapsed_time = time.time() - start_time\n",
        "print(f\"Elapsed time to compute the importances: {elapsed_time:.3f} seconds\")\n",
        "\n",
        "feature_importances = pd.Series(result.importances_mean, index=train_x.columns)\n",
        "print(feature_importances)"
      ],
      "metadata": {
        "id": "BN__i22VDjQ3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
